Common tips for writing scrapers
================================

The following doc contains a list of simple recipies to help scrape data
down from legislative websites. These are by no means the only way to do
these things, but it's the way that we've settled on liking enough to use
in a few places.

Fetching a page and setting URLs to absolute paths
--------------------------------------------------

It's handy to be able to set all the relative URL paths to absolute paths.
``lxml`` has a pretty neat facility for doing this.

It's not uncommon to see a method such as::

    def lxmlize(self, url):
        entry = self.urlopen(url)
        page = lxml.html.fromstring(entry)
        page.make_links_absolute(url)
        return page

being used to proxy calls to ``urlopen`` (since ``scrapelib`` it's self
has no dependency on ``lxml``, this isn't the default behavior).

Getting the current session
---------------------------

The current session will be set (in any ``Scraper`` subclass) to the
``session`` instance var. It's common to see this being used in the ``Event``
objects::

    e = Event(name=name,                                                 
              session=self.session,                                      
              when=when,                                                 
              location='unknown')                                        

This also leads to a common way to gate scrapes (and limit them to the current
session only)::

    if self.session != self.get_current_session():
        raise Exception("We can't scrape past sessions")

Common XPath tricks
-------------------

The following is a small list of very common tricks hackers use in ``xpath``
expressions.

Quick text grabs
++++++++++++++++

Getting text values of HTML elements::

    //some-tag/ul/li/text()

Which would be roughly similar to the following pseudo-code::

    [x.text for x in page.xpath("//some-tag/ul/li")]
    
    # or, more abstractly:

    for el in page.xpath("//some-tag/ul/li"):
        deal_with(el.text)

This is helpful for quickly getting the text values of a bunch of nodes at once
without having to call ``.text`` on all of them. It's worth noting that
this is *different* behavior than ``.text_content()``.

Class limiting / ID limiting
++++++++++++++++++++++++++++

Sometimes it's helpful to get particular nodes of a given class or ID::

    //some-tag[@class='foo']//div[@id='joe']

This expression will find all ``div`` objects with an ``id`` of ``joe`` (I know,
you *should* only use an ``id`` once, but alas sometimes these things happen)
that are sub-nodes of a ``some-tag`` with a ``class`` of ``foo``.

Contains queries
++++++++++++++++

With the above, it's sometimes needed to search for all ``class`` attributes
that *contain* a given string (sometimes sites have quite a bit of autogenerated
stuff around an ID or class name, but a substring stays in place)

Let's take a look at limiting queries::

    //some-tag[contains(@class, 'MainDiv')]

This will find any instance of ``some-tag`` who's ``class`` contains the
substring ``MainDiv``. For example, this *will* match an element such
as ``<some-tag class='FooBar12394MainDiv333' ></some-div>``, but it will
*not* match ``<some-tag class='FooBarMain123Divsf'></some-div>`` or a
``some-tag`` without a class.

Keep in mind that the ``@foo`` can be any attribute of the HTML element,
such as ``@src`` for an ``img`` tag or an ``@href`` for an ``a`` tag.

Array Access
++++++++++++

.. WARNING::
    Be careful with this one!

You can access indexes of returned lists using square brackets (just like in
Python it's self), although this tends to not be advised (since the counts
can often change, and you may end up scraping in bad data).

However, this is sometimes needed::

    //foobar/baz[1]/*

to get all entries under the 1st ``baz`` under a ``foobar``. It's also worth
noting that **xpath indexes are 1-based not 0-based**. Start your counts from
``1`` not ``0`` and you'll have a much better day!
